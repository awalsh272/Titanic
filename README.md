# Titanic
Kaggle's Titanic competition, just for messing around with sklearn, pandas, etc

After some data cleaning and EDA, ran logistic regression and random forests. Would like to try XGBoost and PCA at some point. Additionally, would like to try different imputation methods for missing data, as well as more robust feature selection/engineering. Should perhaps consider transforming some of the variables (log, boxcox), adding interaction terms, addressing multicollinearity between socio-economic status variables (PClass and Fare), and try different combinations of variables. The Random Forest will do some feature selection, so running LR Tests or comparing AIC/BIC for different logistic regression models with different combinations of variables could lead to some insights, as well as improving model accuracy.
